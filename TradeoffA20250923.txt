Here are a few papers about AI agents and world models:
1. The Limits of Predicting Agents from Behaviour https://arxiv.org/abs/2506.02923
2. General agents contain world models https://arxiv.org/abs/2506.01622v1
3. Robust agents learn causal world models https://arxiv.org/html/2402.10877v2
Can you help me write part 1 of the Agda, Coq and Lean4 formalizations of these papers, with unifications and isomorphisms between the paradigms, by formalizing Markov processes and other definitions within these papers, using the category-theoretic definitions of Markov categories, diffeomorphic categories, etc. with required properties? Of course it can be too big for a single response, but just give me something that will parse, at least with placeholders like `admit` and `sorry` if the thing is too big to fit within the context window, but also provide a copiable infobox that summarizes the content and the APIs so that it can be used to formalize further parts by copying into other chats, from now until completion, although you do not need a complete roadmap now, it's just incremental. Can you also give formal sketches with ideas, for example by using the Yoneda lemma and the co-Yoneda lemma to argue that robust agents must work within all possible situations and therefore must have all views to see the world and/or all morphisms to act upon the world, thus satisfying the conditions of the Yoneda and/or the co-Yoneda lemma? What about the third paper? Can you use these ideas to refute the claim that training data should be filtered to prevent misalignment, and can you attempt to formalize the intuition that there is a tradeoff between user-caused harm (misuse) and self-caused harm (misalignment) using the formalisms of these papers? What kind of arguments and theorems would be used? If you can come up with a complete proof, by all means try to do so, else I will return later in other chats. https://www.alignmentforum.org/posts/dEiBJDtSmbC8dChwe/what-training-data-should-developers-filter-to-reduce-risk-1