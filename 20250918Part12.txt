Write Part 12 of the pro-accelerationist position paper on AI development based on the precise formal forms and proofs of Wentworth's good regulator theorem at https://alignmentforum.org/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem and his unification of utility maximization and description length minimization based on prefix codes as used in Huffman and arithmetic coding satisfying Kraft's inequality, allowing representation of probabilities, and his do-divergence theorem at https://www.alignmentforum.org/posts/DHSY697pRWYto6LsF/do-divergence-a-bound-for-maxwell-s-demon as well as the following papers:
1. General agents contain world models https://arxiv.org/abs/2506.01622
2. The limits of predicting agents from behavior https://arxiv.org/abs/2506.02923
3. A black swan hypothesis: the role of human irrationality in AI safety https://arxiv.org/html/2407.18422v1
Please argue that the good regulator theorem, the do-divergence theorem and the paper "General agents contain world models" all show in different languages that control systems must exactly model the thing they want to control, by unifying the mathematical formalisms for information theory, utility maximization, Markov decision processes and do-calculus into one system by equivalence theorems and proofs, and try to mathematically unify the theorems and proofs of the latter two papers about the limits of predicting agents from behavior without data with the former results by treating the "predictor" in the latter two papers as the "agent" in the good regulator theorem and paper 1.
In addition, can you please help me formalize these papers in Agda, Rocq (formerly Coq) and Lean4, by explicitly defining the basic concepts of the paper such as Markov decision processes and agents among others, all the major categories used in these papers, using the same directory/module for the definition of the categories, while separating the propositions, theorems and proofs into their own folders for each paper, by giving the first part of the formalization as far as possible within the context limits and the bound for the pro-accelerationist text, with gaps to be filled in later shown as placeholders like `!!`, `sorry` or `admit`, and with something that can be copied into later chats for formalization of each part in a textbox so that I can request formalization of each file in LLMs later?