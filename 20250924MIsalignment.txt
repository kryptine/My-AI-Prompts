Here are a few papers about optimization theory and machine learning:
1. Clarke Differentials and the Envelope Theorem in Dynamic Programming https://arxiv.org/html/2509.17103v1
2. Linear Regression under Missing or Corrupted Coordinates https://arxiv.org/html/2509.19242v1
3. Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility https://arxiv.org/html/2509.14386v1
4. The Alignment Bottleneck https://arxiv.org/abs/2509.15932v1
5. Murphyâ€™s Laws of AI Alignment: Why the Gap Always Wins https://arxiv.org/html/2509.05381v2
Can you help me write the initial section of the Agda, Rocq (formerly Coq) and Lean4 formalizations of these papers, with unifications and isomorphisms between the paradigms, for example by showing that the theorems and efficient and optimal algorithms for linear regression can be applied to dynamic programming by reparameterizing the entire solution space as a high-dimensional Euclidean space and show an isomorphism between linear regression and dynamic programming, with explicit backward and forward conversion and reparameterization (coordinate change) formulas, in which the efficient algorithms are interconvertible between the two frameworks and in which the envelope theorem is equivalent to the optimality of these algorithms, and use this combined theorem to translate the proofs and arguments in papers 3, 4 and 5 to linear-regression and dynamic-programming language, by setting the alignment and binary supervision problems as multilinear regression and dynamic programming over multiple points on the manifold determined by the pretraining data/base model? Please define information theory, differentials, linear regression, probability theory, and all concepts relevant within these papers to prove the theorems? If it is too complicated to spell out the complete implementations, just use placeholders like `admit` and `sorry`, as long as it parses, with a textbox that I can copy into later sessions in LLMs so that they know what I am doing when I copy each section for further formalization, with summaries, type signatures, and enough formal information to reconstruct the rest, with formal notation preferred over natural language for brevity. Can you please give me the right formal ideas and the right basic theorems necessary to prove these equivalences? In addition, can you use these theorems to argue that there is a tradeoff in AI alignment between externally-mediated harms (misuse) and internally-directed harms (misalignment), by defining or using the right formal concepts among these independent systems to make four independent arguments: based on linear regression for utility/loss functions, dynamic programming, information theory bottlenecks and statistical gaps according to the "Murphy's Law" papers, by a formal mathematical argument with substitutions and inference rules that can use these theorems but ideally should spell out the arguments implicit in the proofs? Can you show me the specific formulas and theorems within the "alignment bottleneck" and "Murphy's Law" papers that state that adversarial = noisy, or else derive them yourself from the formulas present within each paper independently?