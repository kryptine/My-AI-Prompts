Write Part 13 of the pro-accelerationist position paper on AI development based on the precise formal forms and proofs of the following resources by arguing that infra-Bayesianism allows you to act optimally with limited computational or modeling power, by making the arguments based on the formal proofs, theorems and lemmas instead of the general understanding of the concept, making the position paper's argument align with the proofs step-by-step in a way that does not need to necessarily reproduce every step but must still show isomorphisms:
1. https://www.lesswrong.com/posts/pFtCk9xezCssuzvjs/proof-section-to-an-introduction-to-credal-sets-and-infra
2. https://www.lesswrong.com/posts/KRPWJiCfcmdB5smdb/proof-section-to-an-introduction-to-reinforcement-learning
3. https://www.lesswrong.com/posts/2HDydzwuEGE4MNsjx/proof-section-to-crisp-supra-decision-processes
4. Opal: An Operator Algebra View of RLHF https://arxiv.org/html/2509.11298v1
5. Towards a Unified View of Large Language Model Post-Training https://arxiv.org/html/2509.04419v1
In addition, can you please help me formalize these papers in Agda, Rocq (formerly Coq) and Lean4, by explicitly defining the basic concepts of the paper such as Markov decision processes and agents among others, all the major categories used in these papers, using the same directory/module for the definition of the categories, while separating the propositions, theorems and proofs into their own folders for each paper, by giving the first part of the formalization as far as possible within the context limits and the bound for the pro-accelerationist text, with gaps to be filled in later shown as placeholders like `!!`, `sorry` or `admit`, and with something that can be copied into later chats for formalization of each part in a textbox so that I can request formalization of each file in LLMs later?