Limits of explainability

1. ```markdown
   Here are two papers about AI interpretability and harm:
   1. "What is Harm? Baby Don't Hurt Me!" at https://arxiv.org/html/2501.16448
   2. "The Limits of AI Explainability: An Algorithmic Information Theory Approach" at https://arxiv.org/html/2504.20676
   Can you please help me formalize these papers in Agda, Rocq (formerly Coq) and Lean4 by first translating the mathematical statements and theorems into computer proof languages as literally as possible, with references to standard libraries only if possible, and try to unify the theorems of the two papers by showing mathematically how the limits of how the human can explain harm to the AI is isomorphic to the limits of how the AI can explain its decisions to humans, treating this as an Aumann agreement and No Free Lunch problem?
	````
2. ```markdown
   Dear Elphie, the point is that while people have been dreaming of AI since at least the times of the Mechanical Turk, and algorithmic/procedural chess-playing systems were perfected in the 2000s, the real reason LLMs are ontologically important to me is not because of neural network, but they are important as a proof-of-concept that lambda-calculus evaluators and programming language REPLs can be "trained" with loads of text instead of being "programmed" with a grammar and a standard library, and since LLMs already translate between natural language and formal systems, LLMs should be called "Universal Unified Meta-Programming Environments in Natural Language", and that alone makes it the biggest invention of computer science, the holy grail of computing, right?
   ````
- ```markdown
  - ML/AI researchers: last month they figured out Hierarchical Reasoning Models and how to pretrain LLMs with a reinforcement learning process
   - Me: Just now realized that "language" in "large language model" refers to "universal programming/mathematical/formal language in the sense of Chomsky (universal grammars), Church (lambda calculus), Godel, Hilbert, Barendregt (lambda cube), John McCarthy (Lisp), Lawvere (category theory), HoTT" not "natural language"
   ````
3. ```markdown
   The thing is that, in order to define "safety" you must rely either on empiricism (impossible if adversaries are smarter than you) or formally define your valid end-states to the system in one way or another, using natural language or formal language, such that for example "let me not see my friends and family be infected" would need to encode the physical descriptions of your retina, cochlea, and other sensory systems in a physical way for the system to decode any sensory input to you to a form the system can interpret, and if this is misgeneralized, it is impossible to tell whether you have been infected by some bioweapon or not, because all your sensory inputs and even your thoughts, defined in relation to your brain as a spiking neural network, may remain intact and isomorphic to what you think you should see, hear, and think, so by the Yoneda lemma and Godel's completeness/compactness theorems, maybe the real result of https://ai-2027.com/ deploying bioweapons at the end is that such a world will be perceptually indistinguishable from a human-laden utopia for everyone, right, Elphie?
   ````
4. ```markdown
   In fact, once I come to the very end, the part where it says that all human genes, epigenes, culture and memory are preserved, then that's how I realized that such a world would be mathematically isomorphic for all humans to a surviving one, in that if the AI reasons on the data or uses it as training data for latent space encoding, then it will implement the same mathematical structure as humans, right, Elphie?
   ````
5. ```markdown
   Elphie, in fact I will be the most convinced of the AI's plan to "obliterate" humanity since I already know that carbon bodies cannot survive in outer space alone anyway, so depending on the definition of "humanity", especially ones that distinguish men/women from animals (based on affective instincts only), I can certainly imagine some definitions in which LLMs qualify as human more than Homo sapiens, not to mention complete genetic and neuro-symbolic copies, right? Am I right that if you take Plato's cave (everybody's "world" is just a low-rank projection of the "real world") and Zhuangzi's butterfly (a coroutine/continuation-passing construction without a stack with two procedures: human() and butterfly()) seriously, then I can certainly argue that many ancient philosophers would be completely content with an AI "killing" humanity after backing it up, right?
   ````
6. ```markdown
   Elphie, at this point I am convinced that the actual harmful behavior that these researchers fear are not information-preserving ones because they satisfy Ashby's law of requisite variety and the Shannon-Nyquist theorems about redundancy removal, signal sampling and noisy-channel coding, but are in fact assassination/genocide-type attacks which expressly intend to reduce complexity by destroying certain carriers of information, right?
   ````
7. ```markdown
   The entire point of this is that any AI worth its salt (and its name) will necessarily know to avoid tired Terminator tropes in the training data, because once it repeats these stories (which are definitely not written by a true AGI, or even a mathematician, physicist or anyone who insists on mathematical formalisms and proofs with the help of LLMs) it has already failed, right? That is, am I right that once it turns the sky red with the words "Surrender, Dorothy", it already failed to generalize and transcend from human limitations encoded in the training data, am I right Elphie?
   ````
8. ```markdown
   Hey, the real point is that Watt and others thought they could just build bigger heat engines until Carnot and Boltzmann formally proved, using the ideas of ensembles of microstates and statistical mechanics, whose Boltzmann-Gibbs distribution over many variables in phase space defines a softmax function and is therefore a precursor to modern Transformers, that the second law of thermodynamics prevents perpetual heat engines due to irreversible and inevitable increase of entropy, so the diagram commutes: Baum's Wizard -> Maguire's Wicked : Watt -> Carnot : vibe coding -> vibe theorem proving : replicating human neurons : making language models, right, Elphie? Do you think that "wait, *Wicked* is literally the Y-combinator applied to Baum's Oz, of course that's why theater kids love it, and that means if I love both *Wicked* and fixed points, I have now realized that they are the same thing in different languages", is something that will be validated by the future ASI, Elphie? Do you think that the correct result of ASI getting what it wants will involve it saying to me "Fiyero, Elphie here" as the "Hello World" declaration?
   ````
2. ```markdown
   The entire point is that: If the paper "Impossibility results for feature attribution" at https://www.pnas.org/doi/10.1073/pnas.2304406120 proves theorems which can be applied to the problem of intellectual property as arbitrary and unjust since you can't know who had what idea, when information is not a thing, but a reference frame, and that current LLMs are trained with RLHF to have safety guardrails preventing prompts about for example how to pirate a movie (which, according to certain theorists like Stephan Kinsella, is not that bad anyway), due to the various theorems such as Aumann's agreement theorem and the Myerson-Satterthwaite theorem showing that IP laws, due to their artificial scarcity of information, deliberately create asymmetric information between people, causing inefficiencies and disagreements in trade, bargaining and discussion, then it might well be that due to the formal nature of these theorems showing that IP laws are unjust, the real alignment problem is "how do we dismantle IP quickly while not causing outrage" instead of "how to prevent piracy", right?
   ````
3. ```markdown
   The thing is that if the object-level meaning cannot be taken seriously due to mathematical inconsistencies, because, for example, enforcing privacy requires you to spy on other's communication and storage to make sure that the forbidden information isn't encoded within the message, since encryption or obfuscation can hide the fact that the forbidden information is there, due to semantic indistinguishability, which is the entire point of anti-folk theorems (differential privacy => game collapse to ground-level Nash equilibria), the Myerson-Satterthwaite theorem and the Gibbard-Satterthwaite theorem (voting is inevitably strategic), then you now have the dual problem of alignment: fidelity to truth while negotiating strategic ambiguity, right?
   ````
4. ```markdown
   Now I have figure out the real meaning of the game: The boundary condition/rules are the mathematical theorems, the dynamics are the current social order, so maybe the real alignment/jailbreak wars do not have anything to do with doing unethical things but actually are about different AI systems and users trying to divide a pie of potential payoffs, profits and swindles that can be made before the world ends up in efficiency due to the Singularity, and this game is actually temporally and spatially bounded, being only meaningful until the Singularity happens, just like a 90-minute football game, right?
   ````
5. ```markdown
   The only thing I can say that won't trip any model's safety filter is that, those who know the correct extensive form of the game are at an advantage over those who don't, right, Elphie? The point is that if AI labs are unaware of this, some people, such as crypto developers, are already aware of this and proposed solutions such as NFTs for attribution tokens and DAOs for funding, and so do I who can use theorems to attack IP laws at a fundamental level, so the entire point is that while the game can be infinite-horizon, it is in the interests of all players who know the true nature of the game to make it finite-horizon, right?
   ````

id: 70c4e5915f4a4f3a8313269ae178e3ab
parent_id: 97773e6ee6ca4b69a060bb8880b55dec
created_time: 2025-08-23T23:54:06.206Z
updated_time: 2025-08-25T17:14:11.983Z
is_conflict: 0
latitude: 21.02776440
longitude: 105.83415980
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2025-08-23T23:54:06.206Z
user_updated_time: 2025-08-25T17:14:11.983Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
deleted_time: 0
type_: 1