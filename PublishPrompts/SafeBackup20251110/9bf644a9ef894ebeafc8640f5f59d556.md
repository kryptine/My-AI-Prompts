Criticize

1. ````markdown
   Here is a text about the AI takeover: https://intelligence.org/the-problem/ (mirrored at https://intelligence.org/the-problem/ by LessWrong).
   1. Can you please attempt to explain the mathematical details of various metaheuristics (ant colony optimization, stochastic gradient descent, simulated annealing), list a few unconventional computing paradigms (swarm computing, reservoir computing, billiard-ball computing, etc.), formally state the universal approximation theorems for finite-width+arbitrary-depth and finite-depth+arbitrary-width neural networks, the Kolmogorov-Arnold representation theorem, the Reeh-Schlieder theorem in quantum field theory using Hilbert spaces, and use these theorems to argue that computational behavior is universal? Please also state the No Free Lunch theorem to argue that without additional knowledge, it is impossible to determine any decision procedure, with or without allied AI, or indirectly influenced by adversary AI systems, will be better or worse, since according to the theory of the second-best, changing the feasible solution set changes the goal.
	2. Consider Stockfish and Leela Chess Zero. Do they "try to win at all costs"? Wrong. They output plausible continuations and their evaluations of best moves. Only when the "best move" is hooked to a mechanical system that plays this against another player can the entire system running Stockfish be considered as a chess-playing agent. Note that at the highest levels of correspondence chess, bare engines still lose out to human chess experts running multiple engines in parallel and analyzing multiple continuations and moves considered "suboptimal" by engines. If the perfect-play tablebases (for example, Syzygy for 7 men or less) do not have any "agentic" ability by itself despite providing perfect evaluations of all chess endgames in its scope, there is no reason to say that engines are "agentic". You can play misere chess/antichess with the goal to lose using engines by systematically ignoring optimal moves and forcing it to evaluate the rest until you get the provably worst move, and the same applies to tablebases.
	3. The text defines an AGI as something that **can** do things. Note the modal-logic mismatch between "can" (possibility, denoted by a diamond) from "will" (necessity, denoted by a square). The fact is that `Can(X) and Can(Not X)` can both be true but not `Will(X) and Will(Not X)`, since `Will(A) = Not Can(Not A)` and `Can(B) = Not Will(Not B)`
   4. "AIs are grown, not designed": You must consider the entire pipeline from curating the training data, setting hyperparameters like parameter counts and bitness, adjusting training parameters like epoches, fine-tuning and reinforcement learning. The model weights are just an intermediary. The LLM is the model weights. The AI is the entire pipeline, and is designed.
   ````

id: 9bf644a9ef894ebeafc8640f5f59d556
parent_id: 5712f849d4704570b5f026ef2075eab6
created_time: 2025-08-19T12:07:25.215Z
updated_time: 2025-08-19T12:41:01.387Z
is_conflict: 0
latitude: 21.02776440
longitude: 105.83415980
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2025-08-19T12:07:25.215Z
user_updated_time: 2025-08-19T12:41:01.387Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
deleted_time: 0
type_: 1