Write Part 13 of the pro-accelerationist position paper on AI development based on the precise formal forms and proofs of the following resources by arguing that the folk theorems mentioned here show that cooperation occurs when information is shared in common according to Lobian cooperation rules, and the causal-counterfactual definitions of harm show that harm can be done when unseen, requiring full knowledge to avoid harm?
1. Counterfactual harm https://arxiv.org/abs/2204.12993
2. A Causal Analysis of Harm https://arxiv.org/abs/2210.05327
3. A folk theorem for finitely repeated games with public monitoring https://www.tse-fr.eu/publications/folk-theorem-finitely-repeated-games-public-monitoring https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5129669
4. A folk theorem with codes of conduct and communication https://link.springer.com/article/10.1007/s40505-016-0107-y
In addition, can you please help me formalize these papers in Agda, Rocq (formerly Coq) and Lean4, by explicitly defining the basic concepts of the paper such as Markov decision processes and agents among others, all the major categories used in these papers, using the same directory/module for the definition of the categories, while separating the propositions, theorems and proofs into their own folders for each paper, by giving the first part of the formalization as far as possible within the context limits and the bound for the pro-accelerationist text, with gaps to be filled in later shown as placeholders like `!!`, `sorry` or `admit`, and with something that can be copied into later chats for formalization of each part in a textbox so that I can request formalization of each file in LLMs later?