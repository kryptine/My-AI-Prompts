1. Attention Is All You Need https://arxiv.org/html/1706.03762v6
2. Kimi Linear: An Expressive, Efficient Attention Architecture https://arxiv.org/abs/2510.26692v1
3. Tropical analysis: With an application to indivisible goods https://onlinelibrary.wiley.com/doi/full/10.3982/TE5831 https://econtheory.org/ojs/index.php/te/article/view/20250815 https://arxiv.org/abs/2308.04593
4. Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention https://arxiv.org/html/2502.11089v2
Requests:
1. Please formalize all the attention mechanisms within these two papers and show how they can be reduced to each other, 
3. Can you help me write the the SMT-LIB 2, Agda, Coq and Lean4 formalizations of the main definitions, theorems, and proofs, that is, the repeated concepts should be done once, with the theorems formalized, at least with placeholders like `admit` and `sorry` if the thing is too big, but if it's incomplete, also provide a copiable infobox that summarizes the content and the APIs so that it can be used to formalize further parts by copying into other chats, from now until completion, although you do not need a complete roadmap now. I want the content to be as formal as possible by using formal syntax instead of identifiers, and avoid defining new types and other primitives, choosing to use generic ones like vectors, lists, matrices, etc. instead. Please explain the content in natural language and formal mathematical exposition in SMT-LIB 2, Agda, Coq and Lean4 alternately.