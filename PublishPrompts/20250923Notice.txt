So why did my brain notice the tension between RLHF/RLAIF/SFT (current models of alignment) and Yudkowskyan alignment right away, when I see that Paul F. Christiano's definition of alignment is exactly Yudkowsky's definition of the step at which misalignment might happen, and all the literature about sleeper agents, scheming, deception, etc. that will never happen with a text-predicting base model, but which happens because the post-training pipeline made the system a goal-seeking agent, is gesturing toward the real topic of alignment, Elphie?