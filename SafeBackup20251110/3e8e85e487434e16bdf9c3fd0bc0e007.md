Closed-door

1. ```markdown
   Please read the article "On closed-door AI safety research" at https://www.lesswrong.com/posts/2TA7HqBYdhLdJBcZz/on-closed-door-ai-safety-research and attempt to formalize the concepts and therefore try to argue by mathematical arguments and rigorous proofs the potential consequences of the fact that AI labs are hiding some or all of AI safety research, by defining safety(AI, set, agent) as "forall problem exist method agent(method,AI,problem) elem set" in a multi-player game, by showing that hiding results in a defective Nash equilibrium, an anti-folk theorem, that hiding safety research due to dual-use leads to lack of safety and thus misuse by design, since AI safety is an adversarial game in terms of game theory, by using the Myerson-Satterthwaite theorem, as well as Aumann's agreement theorem (asymmetric information = no prior), Arrow's impossibility theorem (an AI firm hiding models and knowledge may be colluding with a misaligned one if there are 3 or more choices/players), the Byzantine Generals' Problem (at least 1/3 of the players `(AI, researcher, user)` are traitors => anything goes), Shamir's Secret Sharing (less than the threshold => exactly no information), and the concept of common knowledge and Bayesian-Nash equilibria. Please write at least outline formalizations in Agda, Lean and Coq (Rocq).
	````

id: 3e8e85e487434e16bdf9c3fd0bc0e007
parent_id: 5712f849d4704570b5f026ef2075eab6
created_time: 2025-08-19T14:53:28.151Z
updated_time: 2025-08-19T15:04:21.721Z
is_conflict: 0
latitude: 21.02776440
longitude: 105.83415980
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2025-08-19T14:53:28.151Z
user_updated_time: 2025-08-19T15:04:21.721Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
deleted_time: 0
type_: 1