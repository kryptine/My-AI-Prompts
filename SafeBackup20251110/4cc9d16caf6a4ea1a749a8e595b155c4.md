Pretraining optimizers

1. ```markdown
   Here are papers about pretraining optimizers: "The Optimiser Hidden in Plain Sight: Training with the Loss Landscapeâ€™s Induced Metric" https://arxiv.org/html/2509.03594v1 and "Fantastic Training Optimizers and Where to Find Them" https://arxiv.org/html/2509.02046v2 so can you please start formalization of  the contents of these papers in Agda, Coq and Lean4 by providing a copy-pastable context with directory trees, APIs, meanings and whatever is needed for copying into other chats to work for formalizing each file, together with a few standard-library imports for real numbers, linear algebra and probability/statistics while defining neural networks (Transformers, SSMs) separately and defining pretraining optimizers as heuristics for neural learning and convergence theorems, as parameterized priors over the sample space parameterized by the architecture and the hyperparameters?

id: 4cc9d16caf6a4ea1a749a8e595b155c4
parent_id: d7e51406d1124ef8b04fd924d6d1b02a
created_time: 2025-09-09T04:10:04.337Z
updated_time: 2025-09-09T08:02:36.842Z
is_conflict: 0
latitude: 21.02776440
longitude: 105.83415980
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2025-09-09T04:10:04.337Z
user_updated_time: 2025-09-09T08:02:36.842Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
deleted_time: 0
type_: 1